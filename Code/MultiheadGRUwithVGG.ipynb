{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21aa423e-0520-4555-b139-d058c829782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                         std=[0.229, 0.224, 0.225])  # ImageNet标准化\n",
    "])\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((40, 40)),  # 先放大\n",
    "    transforms.RandomCrop(32),     # 再随机裁剪\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                         std=[0.229, 0.224, 0.225])  # ImageNet标准化\n",
    "])\n",
    "batch_size=4\n",
    "trainset=torchvision.datasets.CIFAR10(root=r'./',train=True,download=True,transform=transform_train)\n",
    "testset=torchvision.datasets.CIFAR10(root=r'./',train=False,download=True,transform=transform)\n",
    "trainloader=torch.utils.data.DataLoader(dataset=trainset,batch_size=batch_size,shuffle=True,num_workers=2)\n",
    "testloader=torch.utils.data.DataLoader(dataset=testset,batch_size=batch_size,shuffle=False,num_workers=2)\n",
    "classes=('plane','car','bird','cat','deer','dog','frog','horse','ship','truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b51413b-4633-4ef6-836b-eb8ffeaf3367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from torch.nn import MultiheadAttention\n",
    "\n",
    "class build_model(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(build_model, self).__init__()\n",
    "        \n",
    "        # 加载预训练的VGG16\n",
    "        vgg = models.vgg16(weights='IMAGENET1K_V1')\n",
    "        \n",
    "        # 提取不同层级的VGG特征\n",
    "        self.vgg_block1 = nn.Sequential(*list(vgg.features.children())[:5])   # 输出64x32x32\n",
    "        self.vgg_block2 = nn.Sequential(*list(vgg.features.children())[5:10]) # 输出128x16x16\n",
    "        self.vgg_block3 = nn.Sequential(*list(vgg.features.children())[10:17])# 输出256x8x8\n",
    "        self.vgg_block4 = nn.Sequential(*list(vgg.features.children())[17:20]) # 输出512x8x8\n",
    "    \n",
    "        # GRU层\n",
    "        self.gru1 = nn.GRU(input_size=3, hidden_size=64, batch_first=True)\n",
    "        self.gru2 = nn.GRU(input_size=64, hidden_size=128, batch_first=True)\n",
    "        self.gru3 = nn.GRU(input_size=128, hidden_size=256, batch_first=True)\n",
    "        self.gru4 = nn.GRU(input_size=256, hidden_size=512, batch_first=True)\n",
    "        self.gru5 = nn.GRU(input_size=512, hidden_size=256, batch_first=True)\n",
    "\n",
    "         # 解码器GRU层（新增）\n",
    "        self.decoder_gru1 = nn.GRU(input_size=512, hidden_size=256, batch_first=True)  # 从vgg4回到vgg3\n",
    "        self.decoder_gru2 = nn.GRU(input_size=256, hidden_size=128, batch_first=True)  # 从vgg3回到vgg2\n",
    "        self.decoder_gru3 = nn.GRU(input_size=128, hidden_size=64, batch_first=True)  # 从vgg2回到vgg1\n",
    "        self.decoder_gru4 = nn.GRU(input_size=64, hidden_size=3, batch_first=True)   # 从vgg1回到输入\n",
    "        \n",
    "        # 注意力层（每层使用对应的VGG特征）\n",
    "        self.attn1 = MultiheadAttention(embed_dim=64, kdim=64, vdim=64, num_heads=4)\n",
    "        self.attn2 = MultiheadAttention(embed_dim=128, kdim=128, vdim=128, num_heads=8)\n",
    "        self.attn3 = MultiheadAttention(embed_dim=256, kdim=256, vdim=256, num_heads=8)\n",
    "        self.attn4 = MultiheadAttention(embed_dim=512, kdim=512, vdim=512, num_heads=8)\n",
    "        self.attn5 = MultiheadAttention(embed_dim=256, kdim=256, vdim=256, num_heads=8)\n",
    "\n",
    "        # 解码器注意力层（新增）\n",
    "        self.decoder_attn1 = MultiheadAttention(embed_dim=256, kdim=256, vdim=256, num_heads=8)  # vgg3特征\n",
    "        self.decoder_attn2 = MultiheadAttention(embed_dim=128, kdim=128, vdim=128, num_heads=8)  # vgg2特征\n",
    "        self.decoder_attn3 = MultiheadAttention(embed_dim=64, kdim=64, vdim=64, num_heads=4)    # vgg1特征\n",
    "        self.decoder_attn4 = MultiheadAttention(embed_dim=3, kdim=3, vdim=3, num_heads=3)       # 输入特征\n",
    "       \n",
    "        # 层归一化\n",
    "        self.norm1 = nn.LayerNorm(64)\n",
    "        self.norm2 = nn.LayerNorm(128)\n",
    "        self.norm3 = nn.LayerNorm(256)\n",
    "        self.norm4 = nn.LayerNorm(512)\n",
    "        self.norm5 = nn.LayerNorm(256)\n",
    "\n",
    "        # 解码器层归一化（新增）\n",
    "        self.decoder_norm1 = nn.LayerNorm(256)\n",
    "        self.decoder_norm2 = nn.LayerNorm(128)\n",
    "        self.decoder_norm3 = nn.LayerNorm(64)\n",
    "        self.decoder_norm4 = nn.LayerNorm(3)\n",
    "\n",
    "        # 特征融合层（新增）\n",
    "        self.fusion1 = nn.Linear(256 + 256, 256)  # 融合编码器和解码器特征\n",
    "        self.fusion2 = nn.Linear(128 + 128, 128)\n",
    "        self.fusion3 = nn.Linear(64 + 64, 64)\n",
    "\n",
    "        self.middrop=nn.Dropout(0.25)\n",
    "        # 分类器\n",
    "        self.fc1 = nn.Linear(515, 128)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.fc3=nn.Linear(32,num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def _to_sequence(self, features):\n",
    "        \"\"\"将特征图转换为序列\"\"\"\n",
    "        batch_size, channels, height, width = features.size()\n",
    "        seq = features.view(batch_size, channels, -1).permute(0, 2, 1)#[batch,h*w,channels]\n",
    "        return seq\n",
    "\n",
    "\n",
    "    def _adjust_sequence_length(self, seq, target_length):\n",
    "        \"\"\"调整序列长度\"\"\"\n",
    "        # seq: [batch, seq_len, features]\n",
    "        if seq.size(1) == target_length:\n",
    "            return seq\n",
    "        \n",
    "        seq = seq.permute(0, 2, 1)  # [batch, features, seq_len]\n",
    "        if seq.size(2) < target_length:\n",
    "            # 上采样\n",
    "            seq = nn.functional.interpolate(seq, size=target_length, mode='linear', align_corners=False)\n",
    "        else:\n",
    "            # 下采样\n",
    "            seq = nn.AdaptiveAvgPool1d(target_length)(seq)\n",
    "        seq = seq.permute(0, 2, 1)  # [batch, target_length, features]\n",
    "        return seq\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 提取多尺度VGG特征\n",
    "        vgg1 = self.vgg_block1(x)  # [batch, 64, 32, 32]\n",
    "        vgg2 = self.vgg_block2(vgg1)  # [batch, 128, 16, 16]\n",
    "        vgg3 = self.vgg_block3(vgg2)  # [batch, 256, 8, 8]\n",
    "        vgg4 = self.vgg_block4(vgg3) # [batch, 512, 8, 8]\n",
    "        \n",
    "        # 转换为序列\n",
    "        vgg1_seq = self._to_sequence(vgg1)  # [batch, 1024, 64]\n",
    "        vgg2_seq = self._to_sequence(vgg2)  # [batch, 256, 128]\n",
    "        vgg3_seq = self._to_sequence(vgg3)  # [batch, 64, 256]\n",
    "        vgg4_seq = self._to_sequence(vgg4)  # [batch, 64, 512]\n",
    "\n",
    "        # 第一层处理\n",
    "        x_seq = self._to_sequence(x)  \n",
    "        gru1_out, _ = self.gru1(x_seq)  # [batch, 1024, 64]\n",
    "        \n",
    "        # 第一层注意力\n",
    "        query1 = vgg1_seq.permute(1, 0, 2)  # [1024, batch, 64]\n",
    "        key1 = gru1_out.permute(1, 0, 2)     # [1024, batch, 64]\n",
    "        value1 = gru1_out.permute(1, 0, 2)   # [1024, batch, 64]\n",
    "        attn_out1, _ = self.attn1(query1, key1, value1)\n",
    "        attn_out1 = attn_out1.permute(1, 0, 2)  # [batch, 1024, 64]\n",
    "        \n",
    "        # 残差连接 + 归一化\n",
    "        res1 = vgg1_seq + attn_out1\n",
    "        norm1 = self.norm1(res1)\n",
    "        \n",
    "        # 第二层处理\n",
    "        # 调整序列长度\n",
    "        norm1 = norm1.permute(0, 2, 1)  # [batch, 64, 1024] [B,C,L]\n",
    "        norm1 = nn.AdaptiveAvgPool1d(256)(norm1)  # 调整序列长度 [batch,64,256]\n",
    "        norm1 = norm1.permute(0, 2, 1)  # [batch, 256, 64] [B,L,C]\n",
    "        \n",
    "        gru2_out, _ = self.gru2(norm1)  # [batch, 256, 128]\n",
    "        \n",
    "        # 第二层注意力\n",
    "        query2 = vgg2_seq.permute(1,0,2)  # [256, batch, 128]\n",
    "        key2 =  gru2_out.permute(1, 0, 2)    # [256, batch, 128]\n",
    "        value2 =  gru2_out.permute(1, 0, 2)   # [256, batch, 128] \n",
    "        attn_out2, _ = self.attn2(query2, key2, value2)#[256,batch,128]  [长度，batch,维度]\n",
    "        attn_out2 = attn_out2.permute(1, 0, 2)  # [batch, 256, 128]->[batch,长度,维度]\n",
    "        \n",
    "        # 残差连接 + 归一化\n",
    "        res2 = vgg2_seq + attn_out2\n",
    "        norm2 = self.norm2(res2)\n",
    "        \n",
    "        # 第三层处理\n",
    "        # 调整序列长度\n",
    "        norm2 = norm2.permute(0, 2, 1)  # [batch, 128, 256]\n",
    "        norm2 = nn.AdaptiveAvgPool1d(64)(norm2)  # 调整序列长度\n",
    "        norm2 = norm2.permute(0, 2, 1)  # [batch,64,128]\n",
    "        \n",
    "        gru3_out, _ = self.gru3(norm2)  # [batch, 64, 256]\n",
    "        \n",
    "        # 第三层注意力\n",
    "        query3 = vgg3_seq.permute(1, 0, 2)  # [64, batch, 256]\n",
    "        key3 = gru3_out.permute(1, 0, 2)    # [64, batch, 256]\n",
    "        value3 = gru3_out.permute(1, 0, 2)   # [64, batch, 256]\n",
    "        attn_out3, _ = self.attn3(query3, key3, value3)#[64,batch,256]\n",
    "        attn_out3 = attn_out3.permute(1, 0, 2)  # [batch, 64, 256]\n",
    "        \n",
    "        # 残差连接 + 归一化\n",
    "        res3 = vgg3_seq + attn_out3\n",
    "        norm3 = self.norm3(res3)\n",
    "\n",
    "        #第四层\n",
    "        gru4_out,_=self.gru4(norm3) #[batch,64,512]\n",
    "        query4=vgg4_seq.permute(1,0,2)#[64,batch,512]\n",
    "        key4=gru4_out.permute(1,0,2)\n",
    "        value4=gru4_out.permute(1,0,2)\n",
    "        attn_out4,_=self.attn4(query4,key4,value4)#[64,batch,512]\n",
    "        attn_out4=attn_out4.permute(1, 0, 2)#[batch,64,512]\n",
    "\n",
    "        res4=gru4_out+attn_out4\n",
    "        norm4=self.norm4(res4)\n",
    "        encoder_final=norm4\n",
    "\n",
    "\n",
    "        #解码器层\n",
    "        decoder1_out, _ = self.decoder_gru1(norm4)  # [batch, 64, 256]\n",
    "        # 解码器注意力：使用vgg3特征\n",
    "        decoder_query1 = vgg3_seq.permute(1, 0, 2)  # [64, batch, 256]\n",
    "        decoder_key1 = decoder1_out.permute(1, 0, 2)       # [64, batch, 256]\n",
    "        decoder_value1 = decoder1_out.permute(1, 0, 2)      # [64, batch, 256]\n",
    "        decoder_attn_out1, _ = self.decoder_attn1(decoder_query1, decoder_key1, decoder_value1)\n",
    "        decoder_attn_out1 = decoder_attn_out1.permute(1, 0, 2)  # [batch, 64, 256]\n",
    "        \n",
    "        # 残差连接 + 归一化\n",
    "        decoder_res1 = vgg3_seq + decoder_attn_out1 #[batch,64,256]\n",
    "        decoder_norm1 = self.decoder_norm1(decoder_res1)\n",
    "        \n",
    "         # 特征融合：编码器和解码器特征\n",
    "        fused1 = torch.cat([norm3, decoder_norm1], dim=-1)  # [batch, 64, 512]\n",
    "        fused1 = self.fusion1(fused1)  # [batch, 64, 256]\n",
    "\n",
    "        fused1_adj = self._adjust_sequence_length(fused1, 256)  # [batch, 256, 256]\n",
    "\n",
    "        #第二层，使用vgg2特征\n",
    "        decoder2_out, _ = self.decoder_gru2(decoder_norm1)#[batch,256,128]\n",
    "        decoder_query2 = vgg2_seq.permute(1, 0, 2)   #[256,batch,128]\n",
    "        decoder_key2 = decoder2_out.permute(1, 0, 2)       #[256,batch,128]\n",
    "        decoder_value2 = decoder2_out.permute(1, 0, 2)     #[256,batch,128]\n",
    "        decoder_attn_out2, _ = self.decoder_attn2(decoder_query2, decoder_key2, decoder_value2)\n",
    "        decoder_attn_out2 = decoder_attn_out2.permute(1, 0, 2) #[batch,256,128]\n",
    "        # 残差连接 + 归一化\n",
    "        decoder_res2 = vgg2_seq + decoder_attn_out2\n",
    "        decoder_norm2 = self.decoder_norm2(decoder_res2)\n",
    "        # 特征融合\n",
    "        fused2 = torch.cat([norm2, decoder_norm2], dim=-1)  # [batch, 256, 256]\n",
    "        fused2 = self.fusion2(fused2)  # [batch, 256, 128]\n",
    "        \n",
    "        # 第三层解码：从vgg2回到vgg1\n",
    "        fused2_adj = self._adjust_sequence_length(fused2, 1024)  # [batch, 1024, 128]\n",
    "        decoder3_out, _ = self.decoder_gru3(fused2_adj)  # [batch, 1024, 64]\n",
    "        \n",
    "        # 解码器注意力：使用vgg1特征\n",
    "        decoder_query3 = vgg1_seq.permute(1, 0, 2)  # [1024, batch, 64]\n",
    "        decoder_key3 = decoder3_out.permute(1, 0, 2)        # [1024, batch, 64]\n",
    "        decoder_value3 = decoder3_out.permute(1, 0, 2)      # [1024, batch, 64]\n",
    "        decoder_attn_out3, _ = self.decoder_attn3(decoder_query3, decoder_key3, decoder_value3)\n",
    "        decoder_attn_out3 = decoder_attn_out3.permute(1, 0, 2)  # [batch, 1024, 64]\n",
    "        \n",
    "        # 残差连接 + 归一化\n",
    "        decoder_res3 = vgg1_seq + decoder_attn_out3\n",
    "        decoder_norm3 = self.decoder_norm3(decoder_res3)\n",
    "        # 特征融合\n",
    "        fused3 = torch.cat([norm1, decoder_norm3], dim=-1)  # [batch, 1024, 128]\n",
    "        fused3 = self.fusion3(fused3)  # [batch, 1024, 64]\n",
    "        \n",
    "         # 第四层解码：从vgg1回到输入\n",
    "        decoder4_out, _ = self.decoder_gru4(fused3)  # [batch, 1024, 3]\n",
    "        \n",
    "        # 解码器注意力：使用输入特征\n",
    "        decoder_query4 = x_seq.permute(1, 0, 2)  # [1024, batch, 3]\n",
    "        decoder_key4 = decoder4_out.permute(1, 0, 2)           # [1024, batch, 3]\n",
    "        decoder_value4 = decoder4_out.permute(1, 0, 2)        # [1024, batch, 3]\n",
    "        decoder_attn_out4, _ = self.decoder_attn4(decoder_query4, decoder_key4, decoder_value4)\n",
    "        decoder_attn_out4 = decoder_attn_out4.permute(1, 0, 2)  # [batch, 1024, 3]\n",
    "        \n",
    "        # 残差连接 + 归一化\n",
    "        decoder_res4 = x_seq + decoder_attn_out4\n",
    "        decoder_final = self.decoder_norm4(decoder_res4)  # [batch, 1024, 3]\n",
    "\n",
    "         # === 最终特征融合和分类 ===\n",
    "        # 使用编码器最终特征和解码器最终特征\n",
    "        encoder_pooled = torch.mean(encoder_final, dim=1)  # [batch, 512]\n",
    "        decoder_pooled = torch.mean(decoder_final, dim=1)  # [batch, 3]\n",
    "        \n",
    "        # 融合两种特征\n",
    "        final_features = torch.cat([encoder_pooled, decoder_pooled], dim=1)  # [batch, 515]\n",
    "\n",
    "        # 分类器\n",
    "        x=self.relu(self.fc1(final_features))\n",
    "        x = self.fc2(x)\n",
    "        x=self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76fe061f-5431-45d0-8cd6-51f5c7566891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Batch [2000/12500], Loss: 1.5703, Acc: 43.40%\n",
      "Epoch [1/30], Batch [4000/12500], Loss: 1.1840, Acc: 58.25%\n",
      "Epoch [1/30], Batch [6000/12500], Loss: 1.0559, Acc: 63.24%\n",
      "Epoch [1/30], Batch [8000/12500], Loss: 1.0226, Acc: 65.10%\n",
      "Epoch [1/30], Batch [10000/12500], Loss: 0.8487, Acc: 71.09%\n",
      "Epoch [1/30], Batch [12000/12500], Loss: 0.9959, Acc: 65.64%\n",
      "Epoch 1, Validation Loss: 0.9343, Accuracy: 68.87%, LR: 0.000088\n",
      "Epoch [2/30], Batch [2000/12500], Loss: 0.9316, Acc: 68.26%\n",
      "Epoch [2/30], Batch [4000/12500], Loss: 0.8380, Acc: 72.29%\n",
      "Epoch [2/30], Batch [6000/12500], Loss: 0.7598, Acc: 74.40%\n",
      "Epoch [2/30], Batch [8000/12500], Loss: 0.7036, Acc: 76.55%\n",
      "Epoch [2/30], Batch [10000/12500], Loss: 0.9390, Acc: 68.92%\n",
      "Epoch [2/30], Batch [12000/12500], Loss: 0.9176, Acc: 69.08%\n",
      "Epoch 2, Validation Loss: 0.8645, Accuracy: 72.88%, LR: 0.000089\n",
      "Epoch [3/30], Batch [2000/12500], Loss: 0.8520, Acc: 72.05%\n",
      "Epoch [3/30], Batch [4000/12500], Loss: 0.8196, Acc: 73.06%\n",
      "Epoch [3/30], Batch [6000/12500], Loss: 0.7713, Acc: 74.69%\n",
      "Epoch [3/30], Batch [8000/12500], Loss: 0.7137, Acc: 76.59%\n",
      "Epoch [3/30], Batch [10000/12500], Loss: 0.7064, Acc: 77.00%\n",
      "Epoch [3/30], Batch [12000/12500], Loss: 0.6624, Acc: 78.01%\n",
      "Epoch 3, Validation Loss: 0.5934, Accuracy: 81.36%, LR: 0.000008\n",
      "Epoch [4/30], Batch [2000/12500], Loss: 0.6053, Acc: 80.70%\n",
      "Epoch [4/30], Batch [4000/12500], Loss: 0.6799, Acc: 78.21%\n",
      "Epoch [4/30], Batch [6000/12500], Loss: 0.8387, Acc: 72.42%\n",
      "Epoch [4/30], Batch [8000/12500], Loss: 0.8407, Acc: 72.58%\n",
      "Epoch [4/30], Batch [10000/12500], Loss: 0.8465, Acc: 72.03%\n",
      "Epoch [4/30], Batch [12000/12500], Loss: 0.8191, Acc: 73.49%\n",
      "Epoch 4, Validation Loss: 0.8004, Accuracy: 75.27%, LR: 0.000089\n",
      "Epoch [5/30], Batch [2000/12500], Loss: 0.7813, Acc: 74.39%\n",
      "Epoch [5/30], Batch [4000/12500], Loss: 0.7949, Acc: 74.41%\n",
      "Epoch [5/30], Batch [6000/12500], Loss: 0.7577, Acc: 75.01%\n",
      "Epoch [5/30], Batch [8000/12500], Loss: 0.7531, Acc: 75.65%\n",
      "Epoch [5/30], Batch [10000/12500], Loss: 0.7423, Acc: 76.44%\n",
      "Epoch [5/30], Batch [12000/12500], Loss: 0.6988, Acc: 77.61%\n",
      "Epoch 5, Validation Loss: 0.7193, Accuracy: 78.33%, LR: 0.000046\n",
      "Epoch [6/30], Batch [2000/12500], Loss: 0.6564, Acc: 79.62%\n",
      "Epoch [6/30], Batch [4000/12500], Loss: 0.6367, Acc: 80.08%\n",
      "Epoch [6/30], Batch [6000/12500], Loss: 0.6292, Acc: 80.26%\n",
      "Epoch [6/30], Batch [8000/12500], Loss: 0.5820, Acc: 81.62%\n",
      "Epoch [6/30], Batch [10000/12500], Loss: 0.5611, Acc: 82.24%\n",
      "Epoch [6/30], Batch [12000/12500], Loss: 0.5902, Acc: 81.39%\n",
      "Epoch 6, Validation Loss: 0.6072, Accuracy: 82.45%, LR: 0.000008\n",
      "Epoch [7/30], Batch [2000/12500], Loss: 0.5471, Acc: 83.24%\n",
      "Epoch [7/30], Batch [4000/12500], Loss: 0.5276, Acc: 83.90%\n",
      "Epoch [7/30], Batch [6000/12500], Loss: 0.5276, Acc: 83.85%\n",
      "Epoch [7/30], Batch [8000/12500], Loss: 0.6566, Acc: 79.90%\n",
      "Epoch [7/30], Batch [10000/12500], Loss: 0.8025, Acc: 74.41%\n",
      "Epoch [7/30], Batch [12000/12500], Loss: 0.8161, Acc: 73.79%\n",
      "Epoch 7, Validation Loss: 0.8650, Accuracy: 75.22%, LR: 0.000099\n",
      "Epoch [8/30], Batch [2000/12500], Loss: 0.7528, Acc: 74.96%\n",
      "Epoch [8/30], Batch [4000/12500], Loss: 0.7879, Acc: 74.58%\n",
      "Epoch [8/30], Batch [6000/12500], Loss: 0.7893, Acc: 74.50%\n",
      "Epoch [8/30], Batch [8000/12500], Loss: 0.7692, Acc: 75.47%\n",
      "Epoch [8/30], Batch [10000/12500], Loss: 0.7756, Acc: 74.96%\n",
      "Epoch [8/30], Batch [12000/12500], Loss: 0.7613, Acc: 75.88%\n",
      "Epoch 8, Validation Loss: 0.7083, Accuracy: 78.03%, LR: 0.000089\n",
      "Epoch [9/30], Batch [2000/12500], Loss: 0.7609, Acc: 76.03%\n",
      "Epoch [9/30], Batch [4000/12500], Loss: 0.7271, Acc: 76.95%\n",
      "Epoch [9/30], Batch [6000/12500], Loss: 0.7418, Acc: 76.85%\n",
      "Epoch [9/30], Batch [8000/12500], Loss: 0.7335, Acc: 76.88%\n",
      "Epoch [9/30], Batch [10000/12500], Loss: 0.7019, Acc: 77.76%\n",
      "Epoch [9/30], Batch [12000/12500], Loss: 0.7290, Acc: 77.06%\n",
      "Epoch 9, Validation Loss: 0.6764, Accuracy: 79.84%, LR: 0.000070\n",
      "Epoch [10/30], Batch [2000/12500], Loss: 0.6832, Acc: 78.06%\n",
      "Epoch [10/30], Batch [4000/12500], Loss: 0.6746, Acc: 78.72%\n",
      "Epoch [10/30], Batch [6000/12500], Loss: 0.6673, Acc: 79.31%\n",
      "Epoch [10/30], Batch [8000/12500], Loss: 0.6670, Acc: 79.60%\n",
      "Epoch [10/30], Batch [10000/12500], Loss: 0.6510, Acc: 79.89%\n",
      "Epoch [10/30], Batch [12000/12500], Loss: 0.6236, Acc: 81.01%\n",
      "Epoch 10, Validation Loss: 0.8205, Accuracy: 77.30%, LR: 0.000046\n",
      "Epoch [11/30], Batch [2000/12500], Loss: 0.6334, Acc: 80.81%\n",
      "Epoch [11/30], Batch [4000/12500], Loss: 0.6233, Acc: 81.10%\n",
      "Epoch [11/30], Batch [6000/12500], Loss: 0.6121, Acc: 81.46%\n",
      "Epoch [11/30], Batch [8000/12500], Loss: 0.5884, Acc: 81.89%\n",
      "Epoch [11/30], Batch [10000/12500], Loss: 0.5967, Acc: 82.25%\n",
      "Epoch [11/30], Batch [12000/12500], Loss: 0.5711, Acc: 82.50%\n",
      "Epoch 11, Validation Loss: 0.6012, Accuracy: 83.40%, LR: 0.000024\n",
      "Epoch [12/30], Batch [2000/12500], Loss: 0.5496, Acc: 83.49%\n",
      "Epoch [12/30], Batch [4000/12500], Loss: 0.5220, Acc: 84.39%\n",
      "Epoch [12/30], Batch [6000/12500], Loss: 0.5333, Acc: 84.10%\n",
      "Epoch [12/30], Batch [8000/12500], Loss: 0.5316, Acc: 84.41%\n",
      "Epoch [12/30], Batch [10000/12500], Loss: 0.5581, Acc: 83.59%\n",
      "Epoch [12/30], Batch [12000/12500], Loss: 0.5053, Acc: 85.19%\n",
      "Epoch 12, Validation Loss: 0.5791, Accuracy: 84.70%, LR: 0.000008\n",
      "Epoch [13/30], Batch [2000/12500], Loss: 0.5130, Acc: 84.90%\n",
      "Epoch [13/30], Batch [4000/12500], Loss: 0.4952, Acc: 85.49%\n",
      "Epoch [13/30], Batch [6000/12500], Loss: 0.4890, Acc: 85.62%\n",
      "Epoch [13/30], Batch [8000/12500], Loss: 0.5057, Acc: 84.99%\n",
      "Epoch [13/30], Batch [10000/12500], Loss: 0.4762, Acc: 85.99%\n",
      "Epoch [13/30], Batch [12000/12500], Loss: 0.4907, Acc: 85.65%\n",
      "Epoch 13, Validation Loss: 0.5819, Accuracy: 84.77%, LR: 0.000001\n",
      "Epoch [14/30], Batch [2000/12500], Loss: 0.5868, Acc: 83.12%\n",
      "Epoch [14/30], Batch [4000/12500], Loss: 0.7704, Acc: 76.29%\n",
      "Epoch [14/30], Batch [6000/12500], Loss: 0.7537, Acc: 76.40%\n",
      "Epoch [14/30], Batch [8000/12500], Loss: 0.7607, Acc: 76.45%\n",
      "Epoch [14/30], Batch [10000/12500], Loss: 0.7420, Acc: 77.00%\n",
      "Epoch [14/30], Batch [12000/12500], Loss: 0.7763, Acc: 75.88%\n",
      "Epoch 14, Validation Loss: 0.6819, Accuracy: 79.44%, LR: 0.000099\n",
      "Epoch [15/30], Batch [2000/12500], Loss: 0.7678, Acc: 76.00%\n",
      "Epoch [15/30], Batch [4000/12500], Loss: 0.7454, Acc: 76.50%\n",
      "Epoch [15/30], Batch [6000/12500], Loss: 0.7584, Acc: 76.20%\n",
      "Epoch [15/30], Batch [8000/12500], Loss: 0.7461, Acc: 76.24%\n",
      "Epoch [15/30], Batch [10000/12500], Loss: 0.7736, Acc: 75.50%\n",
      "Epoch [15/30], Batch [12000/12500], Loss: 0.7267, Acc: 77.01%\n",
      "Epoch 15, Validation Loss: 0.7846, Accuracy: 77.00%, LR: 0.000095\n",
      "Epoch [16/30], Batch [2000/12500], Loss: 0.7265, Acc: 77.17%\n",
      "Epoch [16/30], Batch [4000/12500], Loss: 0.7361, Acc: 77.00%\n",
      "Epoch [16/30], Batch [6000/12500], Loss: 0.7461, Acc: 76.88%\n",
      "Epoch [16/30], Batch [8000/12500], Loss: 0.7346, Acc: 76.79%\n",
      "Epoch [16/30], Batch [10000/12500], Loss: 0.7246, Acc: 76.95%\n",
      "Epoch [16/30], Batch [12000/12500], Loss: 0.7193, Acc: 77.58%\n",
      "Epoch 16, Validation Loss: 0.6616, Accuracy: 79.97%, LR: 0.000089\n",
      "Epoch [17/30], Batch [2000/12500], Loss: 0.6991, Acc: 78.19%\n",
      "Epoch [17/30], Batch [4000/12500], Loss: 0.7420, Acc: 77.03%\n",
      "Epoch [17/30], Batch [6000/12500], Loss: 0.7063, Acc: 78.66%\n",
      "Epoch [17/30], Batch [8000/12500], Loss: 0.7077, Acc: 77.95%\n",
      "Epoch [17/30], Batch [10000/12500], Loss: 0.7176, Acc: 78.19%\n",
      "Epoch [17/30], Batch [12000/12500], Loss: 0.7114, Acc: 78.03%\n",
      "Epoch 17, Validation Loss: 0.8301, Accuracy: 75.91%, LR: 0.000080\n",
      "Epoch [18/30], Batch [2000/12500], Loss: 0.6922, Acc: 78.47%\n",
      "Epoch [18/30], Batch [4000/12500], Loss: 0.7152, Acc: 77.99%\n",
      "Epoch [18/30], Batch [6000/12500], Loss: 0.7019, Acc: 78.06%\n",
      "Epoch [18/30], Batch [8000/12500], Loss: 0.6811, Acc: 78.91%\n",
      "Epoch [18/30], Batch [10000/12500], Loss: 0.6830, Acc: 79.20%\n",
      "Epoch [18/30], Batch [12000/12500], Loss: 0.6871, Acc: 78.75%\n",
      "Epoch 18, Validation Loss: 0.6814, Accuracy: 79.32%, LR: 0.000070\n",
      "Epoch [19/30], Batch [2000/12500], Loss: 0.6767, Acc: 79.42%\n",
      "Epoch [19/30], Batch [4000/12500], Loss: 0.6716, Acc: 78.97%\n",
      "Epoch [19/30], Batch [6000/12500], Loss: 0.6681, Acc: 79.80%\n",
      "Epoch [19/30], Batch [8000/12500], Loss: 0.6666, Acc: 80.11%\n",
      "Epoch [19/30], Batch [10000/12500], Loss: 0.6923, Acc: 78.97%\n",
      "Epoch [19/30], Batch [12000/12500], Loss: 0.6667, Acc: 79.78%\n",
      "Epoch 19, Validation Loss: 0.6736, Accuracy: 80.66%, LR: 0.000058\n",
      "Epoch [20/30], Batch [2000/12500], Loss: 0.6146, Acc: 81.42%\n",
      "Epoch [20/30], Batch [4000/12500], Loss: 0.6544, Acc: 80.01%\n",
      "Epoch [20/30], Batch [6000/12500], Loss: 0.6548, Acc: 80.06%\n",
      "Epoch [20/30], Batch [8000/12500], Loss: 0.6449, Acc: 80.45%\n",
      "Epoch [20/30], Batch [10000/12500], Loss: 0.6293, Acc: 80.96%\n",
      "Epoch [20/30], Batch [12000/12500], Loss: 0.6357, Acc: 80.84%\n",
      "Epoch 20, Validation Loss: 0.6646, Accuracy: 81.77%, LR: 0.000046\n",
      "Epoch [21/30], Batch [2000/12500], Loss: 0.6469, Acc: 81.09%\n",
      "Epoch [21/30], Batch [4000/12500], Loss: 0.6054, Acc: 82.55%\n",
      "Epoch [21/30], Batch [6000/12500], Loss: 0.6371, Acc: 81.61%\n",
      "Epoch [21/30], Batch [8000/12500], Loss: 0.5972, Acc: 82.58%\n",
      "Epoch [21/30], Batch [10000/12500], Loss: 0.6489, Acc: 80.90%\n",
      "Epoch [21/30], Batch [12000/12500], Loss: 0.5937, Acc: 82.08%\n",
      "Epoch 21, Validation Loss: 0.5964, Accuracy: 84.17%, LR: 0.000035\n",
      "Epoch [22/30], Batch [2000/12500], Loss: 0.5897, Acc: 83.01%\n",
      "Epoch [22/30], Batch [4000/12500], Loss: 0.5894, Acc: 83.21%\n",
      "Epoch [22/30], Batch [6000/12500], Loss: 0.5996, Acc: 83.05%\n",
      "Epoch [22/30], Batch [8000/12500], Loss: 0.6069, Acc: 82.36%\n",
      "Epoch [22/30], Batch [10000/12500], Loss: 0.5864, Acc: 83.19%\n",
      "Epoch [22/30], Batch [12000/12500], Loss: 0.5673, Acc: 84.00%\n",
      "Epoch 22, Validation Loss: 0.6605, Accuracy: 83.62%, LR: 0.000024\n",
      "Epoch [23/30], Batch [2000/12500], Loss: 0.5784, Acc: 83.96%\n",
      "Epoch [23/30], Batch [4000/12500], Loss: 0.5640, Acc: 83.90%\n",
      "Epoch [23/30], Batch [6000/12500], Loss: 0.5574, Acc: 84.54%\n",
      "Epoch [23/30], Batch [8000/12500], Loss: 0.5633, Acc: 83.86%\n",
      "Epoch [23/30], Batch [10000/12500], Loss: 0.5646, Acc: 84.01%\n",
      "Epoch [23/30], Batch [12000/12500], Loss: 0.5630, Acc: 84.80%\n",
      "Epoch 23, Validation Loss: 0.6137, Accuracy: 84.53%, LR: 0.000015\n",
      "Epoch [24/30], Batch [2000/12500], Loss: 0.5089, Acc: 85.65%\n",
      "Epoch [24/30], Batch [4000/12500], Loss: 0.5481, Acc: 85.21%\n",
      "Epoch [24/30], Batch [6000/12500], Loss: 0.5306, Acc: 85.31%\n",
      "Epoch [24/30], Batch [8000/12500], Loss: 0.5400, Acc: 85.39%\n",
      "Epoch [24/30], Batch [10000/12500], Loss: 0.5055, Acc: 86.28%\n",
      "Epoch [24/30], Batch [12000/12500], Loss: 0.5181, Acc: 85.67%\n",
      "Epoch 24, Validation Loss: 0.5750, Accuracy: 85.54%, LR: 0.000008\n",
      "Epoch [25/30], Batch [2000/12500], Loss: 0.4941, Acc: 86.12%\n",
      "Epoch [25/30], Batch [4000/12500], Loss: 0.5218, Acc: 85.56%\n",
      "Epoch [25/30], Batch [6000/12500], Loss: 0.5096, Acc: 86.04%\n",
      "Epoch [25/30], Batch [8000/12500], Loss: 0.4916, Acc: 86.65%\n",
      "Epoch [25/30], Batch [10000/12500], Loss: 0.5236, Acc: 86.08%\n",
      "Epoch [25/30], Batch [12000/12500], Loss: 0.4837, Acc: 86.71%\n",
      "Epoch 25, Validation Loss: 0.6063, Accuracy: 85.08%, LR: 0.000003\n",
      "Epoch [26/30], Batch [2000/12500], Loss: 0.4958, Acc: 86.61%\n",
      "Epoch [26/30], Batch [4000/12500], Loss: 0.4880, Acc: 86.88%\n",
      "Epoch [26/30], Batch [6000/12500], Loss: 0.5091, Acc: 86.42%\n",
      "Epoch [26/30], Batch [8000/12500], Loss: 0.4786, Acc: 86.97%\n",
      "Epoch [26/30], Batch [10000/12500], Loss: 0.5106, Acc: 86.15%\n",
      "Epoch [26/30], Batch [12000/12500], Loss: 0.4712, Acc: 87.21%\n",
      "Epoch 26, Validation Loss: 0.5987, Accuracy: 85.51%, LR: 0.000001\n",
      "Epoch [27/30], Batch [2000/12500], Loss: 0.4719, Acc: 87.56%\n",
      "Epoch [27/30], Batch [4000/12500], Loss: 0.6831, Acc: 80.08%\n",
      "Epoch [27/30], Batch [6000/12500], Loss: 0.7881, Acc: 75.70%\n",
      "Epoch [27/30], Batch [8000/12500], Loss: 0.7900, Acc: 75.54%\n",
      "Epoch [27/30], Batch [10000/12500], Loss: 0.7921, Acc: 75.50%\n",
      "Epoch [27/30], Batch [12000/12500], Loss: 0.7974, Acc: 75.30%\n",
      "Epoch 27, Validation Loss: 0.6542, Accuracy: 79.58%, LR: 0.000100\n",
      "Epoch [28/30], Batch [2000/12500], Loss: 0.7785, Acc: 76.03%\n",
      "Epoch [28/30], Batch [4000/12500], Loss: 0.7641, Acc: 75.83%\n",
      "Epoch [28/30], Batch [6000/12500], Loss: 0.7908, Acc: 74.58%\n",
      "Epoch [28/30], Batch [8000/12500], Loss: 0.7871, Acc: 75.46%\n",
      "Epoch [28/30], Batch [10000/12500], Loss: 0.7701, Acc: 76.05%\n",
      "Epoch [28/30], Batch [12000/12500], Loss: 0.7856, Acc: 75.29%\n",
      "Epoch 28, Validation Loss: 0.8899, Accuracy: 75.93%, LR: 0.000099\n",
      "Epoch [29/30], Batch [2000/12500], Loss: 0.7695, Acc: 76.40%\n",
      "Epoch [29/30], Batch [4000/12500], Loss: 0.7799, Acc: 75.81%\n",
      "Epoch [29/30], Batch [6000/12500], Loss: 0.7796, Acc: 75.54%\n",
      "Epoch [29/30], Batch [8000/12500], Loss: 0.8036, Acc: 74.86%\n",
      "Epoch [29/30], Batch [10000/12500], Loss: 0.7914, Acc: 75.40%\n",
      "Epoch [29/30], Batch [12000/12500], Loss: 0.7752, Acc: 75.83%\n",
      "Epoch 29, Validation Loss: 0.6604, Accuracy: 80.01%, LR: 0.000097\n",
      "Epoch [30/30], Batch [2000/12500], Loss: 0.7884, Acc: 75.50%\n",
      "Epoch [30/30], Batch [4000/12500], Loss: 0.7777, Acc: 75.59%\n",
      "Epoch [30/30], Batch [6000/12500], Loss: 0.7803, Acc: 75.69%\n",
      "Epoch [30/30], Batch [8000/12500], Loss: 0.7762, Acc: 75.84%\n",
      "Epoch [30/30], Batch [10000/12500], Loss: 0.7860, Acc: 75.29%\n",
      "Epoch [30/30], Batch [12000/12500], Loss: 0.7875, Acc: 75.15%\n",
      "Epoch 30, Validation Loss: 0.8714, Accuracy: 76.09%, LR: 0.000095\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "model=build_model().to(device)\n",
    "LR=0.0001\n",
    "criterion=nn.CrossEntropyLoss().cuda()\n",
    "optimizer=optim.Adam(model.parameters(),LR,weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, \n",
    "    T_0=10,     # 第一次重启的周期\n",
    "    T_mult=2,   # 每次重启后周期倍增\n",
    "    eta_min=1e-6  # 最小学习率\n",
    ")\n",
    "max_grad_norm = 1.0  # 梯度裁剪阈值\n",
    "# 添加图像描述变量\n",
    "image_captions = []\n",
    "\n",
    "# 创建存储训练指标的列表\n",
    "train_losses = []\n",
    "val_losses = []  # 新增验证损失列表\n",
    "val_accuracies = []\n",
    "learning_rates = []\n",
    "epochs=30\n",
    "for epoch in range(epochs):\n",
    "    running_loss=0.0\n",
    "    total=0\n",
    "    correct=0\n",
    "    model.train()\n",
    "    # 添加当前epoch的图像描述\n",
    "    epoch_caption = f\"Epoch {epoch+1}/{epochs}: Training in progress...\"\n",
    "    image_captions.append(epoch_caption)\n",
    "    for i,data in enumerate(trainloader,0):\n",
    "        inputs,labels=data\n",
    "        inputs=inputs.cuda()\n",
    "        labels=labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs=model(inputs)\n",
    "        loss=criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        running_loss+=loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        if i % 2000 == 1999:  # 每2000个batch打印一次\n",
    "            train_acc = 100 * correct / total\n",
    "            batch_caption = (f'Epoch [{epoch+1}/{epochs}], Batch [{i+1}/{len(trainloader)}], '\n",
    "                            f'Loss: {running_loss/2000:.4f}, Acc: {train_acc:.2f}%')\n",
    "            print(batch_caption)\n",
    "            image_captions.append(batch_caption)\n",
    "            running_loss = 0.0\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            \n",
    "        # 更新学习率\n",
    "        scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        learning_rates.append(current_lr)\n",
    "\n",
    "     # 验证过程\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_acc = 100 * val_correct / val_total\n",
    "    avg_val_loss = val_loss / len(testloader)\n",
    "    val_losses.append(avg_val_loss)  # 记录验证损失\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    epoch_summary = (f'Epoch {epoch+1}, Validation Loss: {avg_val_loss:.4f}, '\n",
    "                    f'Accuracy: {val_acc:.2f}%, LR: {current_lr:.6f}')\n",
    "    print(epoch_summary)\n",
    "    image_captions.append(epoch_summary)\n",
    "    train_losses.append(val_loss)\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3bab5fa-69d1-4892-ad85-c63a53ac6fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Model saved as final_model.pth\n",
      "Training report and accuracy plot saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# 保存最终模型\n",
    "torch.save(model.state_dict(), 'multiheadgru_model.pth')\n",
    "final_caption = \"Training completed. Model saved as final_model.pth\"\n",
    "print(final_caption)\n",
    "image_captions.append(final_caption)\n",
    "\n",
    "# 绘制测试集准确率变化折线图\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, epochs+1), val_accuracies, 'b-o', linewidth=2)\n",
    "plt.title('Test Accuracy Over Epochs', fontsize=14)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Accuracy (%)', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.xticks(range(1, epochs+1))\n",
    "plt.ylim(0, 100)  # 确保y轴从0到100%\n",
    "\n",
    "# 标记最高准确率\n",
    "max_acc = max(val_accuracies)\n",
    "max_epoch = val_accuracies.index(max_acc) + 1\n",
    "plt.annotate(f'Max: {max_acc:.2f}%', \n",
    "             xy=(max_epoch, max_acc),\n",
    "             xytext=(max_epoch+1, max_acc-5),\n",
    "             arrowprops=dict(facecolor='red', shrink=0.05),\n",
    "             fontsize=12)\n",
    "\n",
    "# 保存图表\n",
    "plt.savefig('accuracy_plot_multiheadgru_3.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 添加图表描述\n",
    "plot_caption = (\"Accuracy Plot: Shows the model's performance improvement on the test set over training epochs. \"\n",
    "               f\"Highest accuracy of {max_acc:.2f}% achieved at epoch {max_epoch}.\")\n",
    "image_captions.append(plot_caption)\n",
    "\n",
    "# 保存所有图像描述到文件\n",
    "with open('training_report.txt', 'w') as f:\n",
    "    for caption in image_captions:\n",
    "        f.write(caption + '\\n')\n",
    "\n",
    "print(\"Training report and accuracy plot saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fa41f5-5c06-4a93-bce3-46b8877b3539",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
