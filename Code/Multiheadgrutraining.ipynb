{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e4b4bc6-00b5-4e23-bb35-f7d40ce94bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import copy\n",
    "# 从训练集的50000个样本中，取49000个作为训练集，剩余1000个作为验证集\n",
    "NUM_TRAIN = 49000\n",
    "\n",
    "# 数据预处理，减去cifar-10数据均值\n",
    "transform_normal = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.4914, 0.4822, 0.4465),(0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "# 数据增强\n",
    "transform_aug = T.Compose([\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "# 加载训练集\n",
    "cifar10_train = dset.CIFAR10('./dataset', train=True, download=True, transform=transform_normal)\n",
    "loader_train = DataLoader(cifar10_train, batch_size=64, sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "# 加载验证集\n",
    "cifar10_val = dset.CIFAR10('./dataset', train=True, download=True, transform=transform_normal)\n",
    "loader_val = DataLoader(cifar10_val, batch_size=64, sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "\n",
    "# 加载测试集\n",
    "cifar10_test = dset.CIFAR10('./dataset', train=False, download=True, transform=transform_normal)\n",
    "loader_test = DataLoader(cifar10_test, batch_size=64)\n",
    "USE_GPU = True\n",
    "dtype = torch.float32\n",
    "print_every = 100\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85c42724-641b-4dd1-9790-b4fa7412a586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证模型在验证集或者测试集上的准确率\n",
    "def check_accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()   # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _,preds = scores.max(1)\n",
    "            num_correct += (preds==y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 *acc ))\n",
    "        return acc\n",
    "def train_model(model, optimizer, accs,model_name,epochs=1, scheduler=None):\n",
    "    '''\n",
    "    Parameters:\n",
    "    - model: A Pytorch Module giving the model to train.\n",
    "    - optimizer: An optimizer object we will use to train the model\n",
    "    - epochs: A Python integer giving the number of epochs to train\n",
    "    Returns: best model\n",
    "    '''\n",
    "    best_model_wts = None\n",
    "    best_acc = 0.0\n",
    "    model = model.to(device=device) # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        for t,(x,y) in enumerate(loader_train):\n",
    "            model.train()   # set model to training mode\n",
    "            x = x.to(device, dtype=dtype)\n",
    "            y = y.to(device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print('Epoch %d, loss=%.4f' % (e, loss.item()))\n",
    "        acc = check_accuracy(loader_val, model)\n",
    "        accs.append(100*acc)\n",
    "        if acc > best_acc:\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            best_acc = acc\n",
    "    print('best_acc:',best_acc)\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    torch.save(model.state_dict(), model_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22bef3e9-dc5a-47d7-bc2b-19962c370f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from torch.nn import MultiheadAttention\n",
    "\n",
    "class build_model(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(build_model, self).__init__()\n",
    "        \n",
    "        # 加载预训练的VGG16\n",
    "        vgg = models.vgg16(weights='IMAGENET1K_V1')\n",
    "        \n",
    "        # 提取不同层级的VGG特征\n",
    "        self.vgg_block1 = nn.Sequential(*list(vgg.features.children())[:5])   # 输出64x32x32\n",
    "        self.vgg_block2 = nn.Sequential(*list(vgg.features.children())[5:10]) # 输出128x16x16\n",
    "        self.vgg_block3 = nn.Sequential(*list(vgg.features.children())[10:17])# 输出256x8x8\n",
    "        self.vgg_block4 = nn.Sequential(*list(vgg.features.children())[17:20]) # 输出512x8x8\n",
    "    \n",
    "        # GRU层\n",
    "        self.gru1 = nn.GRU(input_size=3, hidden_size=64, batch_first=True)\n",
    "        self.gru2 = nn.GRU(input_size=64, hidden_size=128, batch_first=True)\n",
    "        self.gru3 = nn.GRU(input_size=128, hidden_size=256, batch_first=True)\n",
    "        self.gru4 = nn.GRU(input_size=256, hidden_size=512, batch_first=True)\n",
    "        self.gru5 = nn.GRU(input_size=512, hidden_size=256, batch_first=True)\n",
    "\n",
    "         # 解码器GRU层（新增）\n",
    "        self.decoder_gru1 = nn.GRU(input_size=512, hidden_size=256, batch_first=True)  # 从vgg4回到vgg3\n",
    "        self.decoder_gru2 = nn.GRU(input_size=256, hidden_size=128, batch_first=True)  # 从vgg3回到vgg2\n",
    "        self.decoder_gru3 = nn.GRU(input_size=128, hidden_size=64, batch_first=True)  # 从vgg2回到vgg1\n",
    "        self.decoder_gru4 = nn.GRU(input_size=64, hidden_size=3, batch_first=True)   # 从vgg1回到输入\n",
    "        \n",
    "        # 注意力层（每层使用对应的VGG特征）\n",
    "        self.attn1 = MultiheadAttention(embed_dim=64, kdim=64, vdim=64, num_heads=4)\n",
    "        self.attn2 = MultiheadAttention(embed_dim=128, kdim=128, vdim=128, num_heads=8)\n",
    "        self.attn3 = MultiheadAttention(embed_dim=256, kdim=256, vdim=256, num_heads=8)\n",
    "        self.attn4 = MultiheadAttention(embed_dim=512, kdim=512, vdim=512, num_heads=8)\n",
    "        self.attn5 = MultiheadAttention(embed_dim=256, kdim=256, vdim=256, num_heads=8)\n",
    "\n",
    "        # 解码器注意力层（新增）\n",
    "        self.decoder_attn1 = MultiheadAttention(embed_dim=256, kdim=256, vdim=256, num_heads=8)  # vgg3特征\n",
    "        self.decoder_attn2 = MultiheadAttention(embed_dim=128, kdim=128, vdim=128, num_heads=8)  # vgg2特征\n",
    "        self.decoder_attn3 = MultiheadAttention(embed_dim=64, kdim=64, vdim=64, num_heads=4)    # vgg1特征\n",
    "        self.decoder_attn4 = MultiheadAttention(embed_dim=3, kdim=3, vdim=3, num_heads=3)       # 输入特征\n",
    "       \n",
    "        # 层归一化\n",
    "        self.norm1 = nn.LayerNorm(64)\n",
    "        self.norm2 = nn.LayerNorm(128)\n",
    "        self.norm3 = nn.LayerNorm(256)\n",
    "        self.norm4 = nn.LayerNorm(512)\n",
    "        self.norm5 = nn.LayerNorm(256)\n",
    "\n",
    "        # 解码器层归一化（新增）\n",
    "        self.decoder_norm1 = nn.LayerNorm(256)\n",
    "        self.decoder_norm2 = nn.LayerNorm(128)\n",
    "        self.decoder_norm3 = nn.LayerNorm(64)\n",
    "        self.decoder_norm4 = nn.LayerNorm(3)\n",
    "\n",
    "        # 特征融合层（新增）\n",
    "        self.fusion1 = nn.Linear(256 + 256, 256)  # 融合编码器和解码器特征\n",
    "        self.fusion2 = nn.Linear(128 + 128, 128)\n",
    "        self.fusion3 = nn.Linear(64 + 64, 64)\n",
    "\n",
    "        self.middrop=nn.Dropout(0.25)\n",
    "        # 分类器\n",
    "        self.fc1 = nn.Linear(515, 128)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.fc3=nn.Linear(32,num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def _to_sequence(self, features):\n",
    "        \"\"\"将特征图转换为序列\"\"\"\n",
    "        batch_size, channels, height, width = features.size()\n",
    "        seq = features.view(batch_size, channels, -1).permute(0, 2, 1)#[batch,h*w,channels]\n",
    "        return seq\n",
    "\n",
    "\n",
    "    def _adjust_sequence_length(self, seq, target_length):\n",
    "        \"\"\"调整序列长度\"\"\"\n",
    "        # seq: [batch, seq_len, features]\n",
    "        if seq.size(1) == target_length:\n",
    "            return seq\n",
    "        \n",
    "        seq = seq.permute(0, 2, 1)  # [batch, features, seq_len]\n",
    "        if seq.size(2) < target_length:\n",
    "            # 上采样\n",
    "            seq = nn.functional.interpolate(seq, size=target_length, mode='linear', align_corners=False)\n",
    "        else:\n",
    "            # 下采样\n",
    "            seq = nn.AdaptiveAvgPool1d(target_length)(seq)\n",
    "        seq = seq.permute(0, 2, 1)  # [batch, target_length, features]\n",
    "        return seq\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 提取多尺度VGG特征\n",
    "        vgg1 = self.vgg_block1(x)  # [batch, 64, 32, 32]\n",
    "        vgg2 = self.vgg_block2(vgg1)  # [batch, 128, 16, 16]\n",
    "        vgg3 = self.vgg_block3(vgg2)  # [batch, 256, 8, 8]\n",
    "        vgg4 = self.vgg_block4(vgg3) # [batch, 512, 8, 8]\n",
    "        \n",
    "        # 转换为序列\n",
    "        vgg1_seq = self._to_sequence(vgg1)  # [batch, 1024, 64]\n",
    "        vgg2_seq = self._to_sequence(vgg2)  # [batch, 256, 128]\n",
    "        vgg3_seq = self._to_sequence(vgg3)  # [batch, 64, 256]\n",
    "        vgg4_seq = self._to_sequence(vgg4)  # [batch, 64, 512]\n",
    "\n",
    "        # 第一层处理\n",
    "        x_seq = self._to_sequence(x)  \n",
    "        gru1_out, _ = self.gru1(x_seq)  # [batch, 1024, 64]\n",
    "        \n",
    "        # 第一层注意力\n",
    "        query1 = vgg1_seq.permute(1, 0, 2)  # [1024, batch, 64]\n",
    "        key1 = gru1_out.permute(1, 0, 2)     # [1024, batch, 64]\n",
    "        value1 = gru1_out.permute(1, 0, 2)   # [1024, batch, 64]\n",
    "        attn_out1, _ = self.attn1(query1, key1, value1)\n",
    "        attn_out1 = attn_out1.permute(1, 0, 2)  # [batch, 1024, 64]\n",
    "        \n",
    "        # 残差连接 + 归一化\n",
    "        res1 = vgg1_seq + attn_out1\n",
    "        norm1 = self.norm1(res1)\n",
    "        \n",
    "        # 第二层处理\n",
    "        # 调整序列长度\n",
    "        norm1 = norm1.permute(0, 2, 1)  # [batch, 64, 1024] [B,C,L]\n",
    "        norm1 = nn.AdaptiveAvgPool1d(256)(norm1)  # 调整序列长度 [batch,64,256]\n",
    "        norm1 = norm1.permute(0, 2, 1)  # [batch, 256, 64] [B,L,C]\n",
    "        \n",
    "        gru2_out, _ = self.gru2(norm1)  # [batch, 256, 128]\n",
    "        \n",
    "        # 第二层注意力\n",
    "        query2 = vgg2_seq.permute(1,0,2)  # [256, batch, 128]\n",
    "        key2 =  gru2_out.permute(1, 0, 2)    # [256, batch, 128]\n",
    "        value2 =  gru2_out.permute(1, 0, 2)   # [256, batch, 128] \n",
    "        attn_out2, _ = self.attn2(query2, key2, value2)#[256,batch,128]  [长度，batch,维度]\n",
    "        attn_out2 = attn_out2.permute(1, 0, 2)  # [batch, 256, 128]->[batch,长度,维度]\n",
    "        \n",
    "        # 残差连接 + 归一化\n",
    "        res2 = vgg2_seq + attn_out2\n",
    "        norm2 = self.norm2(res2)\n",
    "        \n",
    "        # 第三层处理\n",
    "        # 调整序列长度\n",
    "        norm2 = norm2.permute(0, 2, 1)  # [batch, 128, 256]\n",
    "        norm2 = nn.AdaptiveAvgPool1d(64)(norm2)  # 调整序列长度\n",
    "        norm2 = norm2.permute(0, 2, 1)  # [batch,64,128]\n",
    "        \n",
    "        gru3_out, _ = self.gru3(norm2)  # [batch, 64, 256]\n",
    "        \n",
    "        # 第三层注意力\n",
    "        query3 = vgg3_seq.permute(1, 0, 2)  # [64, batch, 256]\n",
    "        key3 = gru3_out.permute(1, 0, 2)    # [64, batch, 256]\n",
    "        value3 = gru3_out.permute(1, 0, 2)   # [64, batch, 256]\n",
    "        attn_out3, _ = self.attn3(query3, key3, value3)#[64,batch,256]\n",
    "        attn_out3 = attn_out3.permute(1, 0, 2)  # [batch, 64, 256]\n",
    "        \n",
    "        # 残差连接 + 归一化\n",
    "        res3 = vgg3_seq + attn_out3\n",
    "        norm3 = self.norm3(res3)\n",
    "\n",
    "        #第四层\n",
    "        gru4_out,_=self.gru4(norm3) #[batch,64,512]\n",
    "        query4=vgg4_seq.permute(1,0,2)#[64,batch,512]\n",
    "        key4=gru4_out.permute(1,0,2)\n",
    "        value4=gru4_out.permute(1,0,2)\n",
    "        attn_out4,_=self.attn4(query4,key4,value4)#[64,batch,512]\n",
    "        attn_out4=attn_out4.permute(1, 0, 2)#[batch,64,512]\n",
    "\n",
    "        res4=gru4_out+attn_out4\n",
    "        norm4=self.norm4(res4)\n",
    "        encoder_final=norm4\n",
    "\n",
    "\n",
    "        #解码器层\n",
    "        decoder1_out, _ = self.decoder_gru1(norm4)  # [batch, 64, 256]\n",
    "        # 解码器注意力：使用vgg3特征\n",
    "        decoder_query1 = vgg3_seq.permute(1, 0, 2)  # [64, batch, 256]\n",
    "        decoder_key1 = decoder1_out.permute(1, 0, 2)       # [64, batch, 256]\n",
    "        decoder_value1 = decoder1_out.permute(1, 0, 2)      # [64, batch, 256]\n",
    "        decoder_attn_out1, _ = self.decoder_attn1(decoder_query1, decoder_key1, decoder_value1)\n",
    "        decoder_attn_out1 = decoder_attn_out1.permute(1, 0, 2)  # [batch, 64, 256]\n",
    "        \n",
    "        # 残差连接 + 归一化\n",
    "        decoder_res1 = vgg3_seq + decoder_attn_out1 #[batch,64,256]\n",
    "        decoder_norm1 = self.decoder_norm1(decoder_res1)\n",
    "        \n",
    "         # 特征融合：编码器和解码器特征\n",
    "        fused1 = torch.cat([norm3, decoder_norm1], dim=-1)  # [batch, 64, 512]\n",
    "        fused1 = self.fusion1(fused1)  # [batch, 64, 256]\n",
    "\n",
    "        fused1_adj = self._adjust_sequence_length(fused1, 256)  # [batch, 256, 256]\n",
    "\n",
    "        #第二层，使用vgg2特征\n",
    "        decoder2_out, _ = self.decoder_gru2(decoder_norm1)#[batch,256,128]\n",
    "        decoder_query2 = vgg2_seq.permute(1, 0, 2)   #[256,batch,128]\n",
    "        decoder_key2 = decoder2_out.permute(1, 0, 2)       #[256,batch,128]\n",
    "        decoder_value2 = decoder2_out.permute(1, 0, 2)     #[256,batch,128]\n",
    "        decoder_attn_out2, _ = self.decoder_attn2(decoder_query2, decoder_key2, decoder_value2)\n",
    "        decoder_attn_out2 = decoder_attn_out2.permute(1, 0, 2) #[batch,256,128]\n",
    "        # 残差连接 + 归一化\n",
    "        decoder_res2 = vgg2_seq + decoder_attn_out2\n",
    "        decoder_norm2 = self.decoder_norm2(decoder_res2)\n",
    "        # 特征融合\n",
    "        fused2 = torch.cat([norm2, decoder_norm2], dim=-1)  # [batch, 256, 256]\n",
    "        fused2 = self.fusion2(fused2)  # [batch, 256, 128]\n",
    "        \n",
    "        # 第三层解码：从vgg2回到vgg1\n",
    "        fused2_adj = self._adjust_sequence_length(fused2, 1024)  # [batch, 1024, 128]\n",
    "        decoder3_out, _ = self.decoder_gru3(fused2_adj)  # [batch, 1024, 64]\n",
    "        \n",
    "        # 解码器注意力：使用vgg1特征\n",
    "        decoder_query3 = vgg1_seq.permute(1, 0, 2)  # [1024, batch, 64]\n",
    "        decoder_key3 = decoder3_out.permute(1, 0, 2)        # [1024, batch, 64]\n",
    "        decoder_value3 = decoder3_out.permute(1, 0, 2)      # [1024, batch, 64]\n",
    "        decoder_attn_out3, _ = self.decoder_attn3(decoder_query3, decoder_key3, decoder_value3)\n",
    "        decoder_attn_out3 = decoder_attn_out3.permute(1, 0, 2)  # [batch, 1024, 64]\n",
    "        \n",
    "        # 残差连接 + 归一化\n",
    "        decoder_res3 = vgg1_seq + decoder_attn_out3\n",
    "        decoder_norm3 = self.decoder_norm3(decoder_res3)\n",
    "        # 特征融合\n",
    "        fused3 = torch.cat([norm1, decoder_norm3], dim=-1)  # [batch, 1024, 128]\n",
    "        fused3 = self.fusion3(fused3)  # [batch, 1024, 64]\n",
    "        \n",
    "         # 第四层解码：从vgg1回到输入\n",
    "        decoder4_out, _ = self.decoder_gru4(fused3)  # [batch, 1024, 3]\n",
    "        \n",
    "        # 解码器注意力：使用输入特征\n",
    "        decoder_query4 = x_seq.permute(1, 0, 2)  # [1024, batch, 3]\n",
    "        decoder_key4 = decoder4_out.permute(1, 0, 2)           # [1024, batch, 3]\n",
    "        decoder_value4 = decoder4_out.permute(1, 0, 2)        # [1024, batch, 3]\n",
    "        decoder_attn_out4, _ = self.decoder_attn4(decoder_query4, decoder_key4, decoder_value4)\n",
    "        decoder_attn_out4 = decoder_attn_out4.permute(1, 0, 2)  # [batch, 1024, 3]\n",
    "        \n",
    "        # 残差连接 + 归一化\n",
    "        decoder_res4 = x_seq + decoder_attn_out4\n",
    "        decoder_final = self.decoder_norm4(decoder_res4)  # [batch, 1024, 3]\n",
    "\n",
    "         # === 最终特征融合和分类 ===\n",
    "        # 使用编码器最终特征和解码器最终特征\n",
    "        encoder_pooled = torch.mean(encoder_final, dim=1)  # [batch, 512]\n",
    "        decoder_pooled = torch.mean(decoder_final, dim=1)  # [batch, 3]\n",
    "        \n",
    "        # 融合两种特征\n",
    "        final_features = torch.cat([encoder_pooled, decoder_pooled], dim=1)  # [batch, 515]\n",
    "        \n",
    "        # 分类器\n",
    "        x=self.relu(self.fc1(final_features))\n",
    "        x = self.fc2(x)\n",
    "        x=self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60c2836e-1dbe-4170-9e73-7853b54706ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "sampler option is mutually exclusive with shuffle",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 训练更多代数，并应用学习率衰减\u001b[39;00m\n\u001b[32m      2\u001b[39m cifar10_train = dset.CIFAR10(\u001b[33m'\u001b[39m\u001b[33m./dataset\u001b[39m\u001b[33m'\u001b[39m, train=\u001b[38;5;28;01mTrue\u001b[39;00m, download=\u001b[38;5;28;01mTrue\u001b[39;00m, transform=transform_aug)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m loader_train = \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcifar10_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampler\u001b[49m\u001b[43m=\u001b[49m\u001b[43msampler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSubsetRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mNUM_TRAIN\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m learning_rate = \u001b[32m1e-2\u001b[39m\n\u001b[32m      5\u001b[39m accs=[]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\conda_envs\\testenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:362\u001b[39m, in \u001b[36mDataLoader.__init__\u001b[39m\u001b[34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device, in_order)\u001b[39m\n\u001b[32m    359\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind = _DatasetKind.Map\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sampler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m shuffle:\n\u001b[32m--> \u001b[39m\u001b[32m362\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33msampler option is mutually exclusive with shuffle\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_sampler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    365\u001b[39m     \u001b[38;5;66;03m# auto_collation with custom batch_sampler\u001b[39;00m\n\u001b[32m    366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m batch_size != \u001b[32m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m shuffle \u001b[38;5;129;01mor\u001b[39;00m sampler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m drop_last:\n",
      "\u001b[31mValueError\u001b[39m: sampler option is mutually exclusive with shuffle"
     ]
    }
   ],
   "source": [
    "# 训练更多代数，并应用学习率衰减\n",
    "cifar10_train = dset.CIFAR10('./dataset', train=True, download=True, transform=transform_aug)\n",
    "loader_train = DataLoader(cifar10_train, batch_size=64, sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "learning_rate = 1e-2\n",
    "accs=[]\n",
    "model = build_model()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=15,gamma=0.1)\n",
    "best_model= train_model(model, optimizer,accs,'multiheadgru.pth',50, scheduler)\n",
    "val_acc=check_accuracy(loader_test, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0123de3c-5949-456c-a97b-4eead7cc09ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs = np.arange(0,50 )\n",
    "# 计算每条曲线最大值位置\n",
    "max_idx_gru = np.argmax(accs)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# 绘制三条曲线\n",
    "plt.plot(epochs, accs, '-o', color='red', label='MultiHeadGRU')\n",
    "\n",
    "# 标注最大值点\n",
    "plt.scatter(epochs[max_idx_gru], accs[max_idx_gru], color='red', s=100, marker='^')\n",
    "\n",
    "# 调整最大值标签位置，避免重叠\n",
    "plt.text(epochs[max_idx_gru]+0.5, accs[max_idx_gru]+0.5, f\"Max: {acc_multiheadgru[max_idx_gru]:.2f}%\", color='red')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.ylim(50, 100)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.savefig('multiheadgru.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
