{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66ed8240-3c2d-407f-9f74-6bb23f59fcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import einsum\n",
    "import math\n",
    "from einops import rearrange, repeat\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                         std=[0.229, 0.224, 0.225])  # ImageNet标准化\n",
    "])\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((40, 40)),  # 先放大\n",
    "    transforms.RandomCrop(32),     # 再随机裁剪\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                         std=[0.229, 0.224, 0.225])  # ImageNet标准化\n",
    "])\n",
    "batch_size=4\n",
    "trainset=torchvision.datasets.CIFAR10(root=r'./',train=True,download=True,transform=transform_train)\n",
    "testset=torchvision.datasets.CIFAR10(root=r'./',train=False,download=True,transform=transform)\n",
    "trainloader=torch.utils.data.DataLoader(dataset=trainset,batch_size=batch_size,shuffle=True,num_workers=2)\n",
    "testloader=torch.utils.data.DataLoader(dataset=testset,batch_size=batch_size,shuffle=False,num_workers=2)\n",
    "classes=('plane','car','bird','cat','deer','dog','frog','horse','ship','truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11aa0a35-e08b-464e-a80d-c8e86a833256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from torch.nn import MultiheadAttention\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"将图像分割成patch并嵌入\"\"\"\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=3, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, \n",
    "                             kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "        # 可学习的位置编码\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches + 1, embed_dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        # 投影到patch嵌入\n",
    "        x = self.proj(x)  # [B, embed_dim, H/patch, W/patch]\n",
    "        x = x.flatten(2).transpose(1, 2)  # [B, num_patches, embed_dim]\n",
    "        \n",
    "        # 添加cls token\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        \n",
    "        # 添加位置编码\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c9e883a-c358-4750-b1bd-78cde22ff1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"多头自注意力机制\"\"\"\n",
    "    def __init__(self, embed_dim=256, num_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        assert self.head_dim * num_heads == embed_dim, \"embed_dim必须能被num_heads整除\"\n",
    "        \n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_drop = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        # 生成Q, K, V\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        # 计算注意力权重\n",
    "        attn = (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        \n",
    "        # 应用注意力权重\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37fcb39f-8d0e-4643-971c-96a77df37d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"多层感知机\"\"\"\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "944ed12c-3765-408a-a4df-b83363a6fc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer模块\"\"\"\n",
    "    def __init__(self, embed_dim=256, num_heads=8, mlp_ratio=4.0, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n",
    "        self.mlp = MLP(embed_dim, mlp_hidden_dim, dropout=dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 残差连接 + 层归一化 + 注意力\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        # 残差连接 + 层归一化 + MLP\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6508c3b-3694-4760-82b9-5c0ac1f3652a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"完整的Vision Transformer模型\"\"\"\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=3, num_classes=10,\n",
    "                 embed_dim=256, depth=6, num_heads=8, mlp_ratio=4.0, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        self.cls_token = self.patch_embed.cls_token\n",
    "        \n",
    "        # Transformer层\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "        # 初始化权重\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # Patch嵌入\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        # 通过Transformer层\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "            \n",
    "        # 层归一化\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # 使用cls token进行分类\n",
    "        cls_token_final = x[:, 0]\n",
    "        x = self.head(cls_token_final)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44a89118-1685-46f8-9c31-b2d05dc64860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建模型实例\n",
    "def create_vit_cifar10(model_name='small'):\n",
    "    \"\"\"创建不同规模的ViT模型\"\"\"\n",
    "    configs = {\n",
    "        'tiny': {'embed_dim': 192, 'depth': 6, 'num_heads': 6},\n",
    "        'small': {'embed_dim': 256, 'depth': 8, 'num_heads': 8},\n",
    "        'base': {'embed_dim': 384, 'depth': 12, 'num_heads': 12}\n",
    "    }\n",
    "    \n",
    "    config = configs.get(model_name, configs['small'])\n",
    "    \n",
    "    model = VisionTransformer(\n",
    "        img_size=32,\n",
    "        patch_size=4,\n",
    "        in_channels=3,\n",
    "        num_classes=10,\n",
    "        embed_dim=config['embed_dim'],\n",
    "        depth=config['depth'],\n",
    "        num_heads=config['num_heads'],\n",
    "        mlp_ratio=4.0,\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9fc3f507-2886-49a0-9aa5-b4ec5a1ae923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Batch [2000/12500], Loss: 2.1268, Acc: 19.06%\n",
      "Epoch [1/30], Batch [4000/12500], Loss: 2.0096, Acc: 24.31%\n",
      "Epoch [1/30], Batch [6000/12500], Loss: 1.9687, Acc: 25.77%\n",
      "Epoch [1/30], Batch [8000/12500], Loss: 1.9362, Acc: 28.30%\n",
      "Epoch [1/30], Batch [10000/12500], Loss: 1.8778, Acc: 30.85%\n",
      "Epoch [1/30], Batch [12000/12500], Loss: 1.8916, Acc: 29.02%\n",
      "Epoch 1, Validation Loss: 1.7575, Accuracy: 36.21%, LR: 0.000088\n",
      "Epoch [2/30], Batch [2000/12500], Loss: 1.8340, Acc: 32.06%\n",
      "Epoch [2/30], Batch [4000/12500], Loss: 1.7986, Acc: 34.30%\n",
      "Epoch [2/30], Batch [6000/12500], Loss: 1.7576, Acc: 34.95%\n",
      "Epoch [2/30], Batch [8000/12500], Loss: 1.7419, Acc: 36.40%\n",
      "Epoch [2/30], Batch [10000/12500], Loss: 1.7966, Acc: 34.24%\n",
      "Epoch [2/30], Batch [12000/12500], Loss: 1.7821, Acc: 34.74%\n",
      "Epoch 2, Validation Loss: 1.6334, Accuracy: 41.22%, LR: 0.000089\n",
      "Epoch [3/30], Batch [2000/12500], Loss: 1.7372, Acc: 35.90%\n",
      "Epoch [3/30], Batch [4000/12500], Loss: 1.6890, Acc: 38.41%\n",
      "Epoch [3/30], Batch [6000/12500], Loss: 1.6912, Acc: 38.73%\n",
      "Epoch [3/30], Batch [8000/12500], Loss: 1.6787, Acc: 39.41%\n",
      "Epoch [3/30], Batch [10000/12500], Loss: 1.6351, Acc: 40.40%\n",
      "Epoch [3/30], Batch [12000/12500], Loss: 1.6467, Acc: 39.99%\n",
      "Epoch 3, Validation Loss: 1.5944, Accuracy: 43.68%, LR: 0.000008\n",
      "Epoch [4/30], Batch [2000/12500], Loss: 1.6190, Acc: 41.59%\n",
      "Epoch [4/30], Batch [4000/12500], Loss: 1.6374, Acc: 40.42%\n",
      "Epoch [4/30], Batch [6000/12500], Loss: 1.6885, Acc: 38.06%\n",
      "Epoch [4/30], Batch [8000/12500], Loss: 1.6734, Acc: 39.94%\n",
      "Epoch [4/30], Batch [10000/12500], Loss: 1.6652, Acc: 39.55%\n",
      "Epoch [4/30], Batch [12000/12500], Loss: 1.6555, Acc: 39.40%\n",
      "Epoch 4, Validation Loss: 1.6738, Accuracy: 40.77%, LR: 0.000089\n",
      "Epoch [5/30], Batch [2000/12500], Loss: 1.6472, Acc: 40.31%\n",
      "Epoch [5/30], Batch [4000/12500], Loss: 1.6407, Acc: 41.48%\n",
      "Epoch [5/30], Batch [6000/12500], Loss: 1.6134, Acc: 41.15%\n",
      "Epoch [5/30], Batch [8000/12500], Loss: 1.5859, Acc: 42.54%\n",
      "Epoch [5/30], Batch [10000/12500], Loss: 1.5698, Acc: 43.76%\n",
      "Epoch [5/30], Batch [12000/12500], Loss: 1.5589, Acc: 43.88%\n",
      "Epoch 5, Validation Loss: 1.6070, Accuracy: 44.21%, LR: 0.000046\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     35\u001b[39m outputs=model(inputs)\n\u001b[32m     36\u001b[39m loss=criterion(outputs,labels)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n\u001b[32m     39\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\conda_envs\\testenv\\Lib\\site-packages\\torch\\_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\conda_envs\\testenv\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\conda_envs\\testenv\\Lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "model = create_vit_cifar10('tiny')\n",
    "LR=0.0001\n",
    "criterion=nn.CrossEntropyLoss().cuda()\n",
    "optimizer=optim.Adam(model.parameters(),LR,weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, \n",
    "    T_0=10,     # 第一次重启的周期\n",
    "    T_mult=2,   # 每次重启后周期倍增\n",
    "    eta_min=1e-6  # 最小学习率\n",
    ")\n",
    "max_grad_norm = 1.0  # 梯度裁剪阈值\n",
    "# 添加图像描述变量\n",
    "image_captions = []\n",
    "\n",
    "# 创建存储训练指标的列表\n",
    "train_losses = []\n",
    "val_losses = []  # 新增验证损失列表\n",
    "val_accuracies = []\n",
    "learning_rates = []\n",
    "epochs=30\n",
    "for epoch in range(epochs):\n",
    "    running_loss=0.0\n",
    "    total=0\n",
    "    correct=0\n",
    "    model.train()\n",
    "    # 添加当前epoch的图像描述\n",
    "    epoch_caption = f\"Epoch {epoch+1}/{epochs}: Training in progress...\"\n",
    "    image_captions.append(epoch_caption)\n",
    "    for i,data in enumerate(trainloader,0):\n",
    "        inputs,labels=data\n",
    "        inputs=inputs.cuda()\n",
    "        labels=labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs=model(inputs)\n",
    "        loss=criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        running_loss+=loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        if i % 2000 == 1999:  # 每2000个batch打印一次\n",
    "            train_acc = 100 * correct / total\n",
    "            batch_caption = (f'Epoch [{epoch+1}/{epochs}], Batch [{i+1}/{len(trainloader)}], '\n",
    "                            f'Loss: {running_loss/2000:.4f}, Acc: {train_acc:.2f}%')\n",
    "            print(batch_caption)\n",
    "            image_captions.append(batch_caption)\n",
    "            running_loss = 0.0\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            \n",
    "        # 更新学习率\n",
    "        scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        learning_rates.append(current_lr)\n",
    "\n",
    "     # 验证过程\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_acc = 100 * val_correct / val_total\n",
    "    avg_val_loss = val_loss / len(testloader)\n",
    "    val_losses.append(avg_val_loss)  # 记录验证损失\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    epoch_summary = (f'Epoch {epoch+1}, Validation Loss: {avg_val_loss:.4f}, '\n",
    "                    f'Accuracy: {val_acc:.2f}%, LR: {current_lr:.6f}')\n",
    "    print(epoch_summary)\n",
    "    image_captions.append(epoch_summary)\n",
    "    train_losses.append(val_loss)\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b13d2c-1f57-4184-b8ea-569fa1a8db04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存最终模型\n",
    "torch.save(model.state_dict(), 'ViT2.pth')\n",
    "final_caption = \"Training completed. Model saved as final_model.pth\"\n",
    "print(final_caption)\n",
    "image_captions.append(final_caption)\n",
    "\n",
    "# 绘制测试集准确率变化折线图\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, epochs+1), val_accuracies, 'b-o', linewidth=2)\n",
    "plt.title('Test Accuracy Over Epochs', fontsize=14)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Accuracy (%)', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.xticks(range(1, epochs+1))\n",
    "plt.ylim(0, 100)  # 确保y轴从0到100%\n",
    "\n",
    "# 标记最高准确率\n",
    "max_acc = max(val_accuracies)\n",
    "max_epoch = val_accuracies.index(max_acc) + 1\n",
    "plt.annotate(f'Max: {max_acc:.2f}%', \n",
    "             xy=(max_epoch, max_acc),\n",
    "             xytext=(max_epoch+1, max_acc-5),\n",
    "             arrowprops=dict(facecolor='red', shrink=0.05),\n",
    "             fontsize=12)\n",
    "\n",
    "# 保存图表\n",
    "plt.savefig('ViT.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 添加图表描述\n",
    "plot_caption = (\"Accuracy Plot: Shows the model's performance improvement on the test set over training epochs. \"\n",
    "               f\"Highest accuracy of {max_acc:.2f}% achieved at epoch {max_epoch}.\")\n",
    "image_captions.append(plot_caption)\n",
    "\n",
    "# 保存所有图像描述到文件\n",
    "with open('training_report.txt', 'w') as f:\n",
    "    for caption in image_captions:\n",
    "        f.write(caption + '\\n')\n",
    "\n",
    "print(\"Training report and accuracy plot saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
